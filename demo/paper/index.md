# EM算法及高斯混合模型GMM详述

2020-04-08阅读 6300

## 1、最大似然估计

最大似然估计（Maximum Likelihood Estimation，MLE）就是利用已知的样本结果，反推最有可能（最大概率）导致这样结果的参数值的计算过程。直白来讲，就是给定了一定的数据，假定知道数据是从某种分布中随机抽取出来的，但是不知道这个分布具体的参数值，即“模型已定，参数未知”，MLE就可以用来估计模型的参数。MLE的目标是找出一组参数（模型中的参数），使得模型产出观察数据的概率最大。

![](assets/1637136132-de6b0640516472690df30636b2b2fc2f.png)

##### MLE求解过程：

*   似然函数，即在样本固定的情况下，样本出现的概率与参数θ之间的函数
*   对似然函数取对数
*   对数似然函数求导数
*   解似然方程

![](assets/1637136132-ffd9799bbc3aafb052895fc0f04986d3.png)

##### 最大后验概率估计

最大后验概率估计（Maximum A Posteriori Estimation，MAP）和MLE一样，都是通过样本估计参数

![](assets/1637136132-fc216086d4162cb46754fd5815e111cb.png)

的值。在MLE中，是使似然函数

![](assets/1637136132-3d4129d61fcfa421715ff9c87e288b5e.png)

最大的时候参数θ\\thetaθ的值，MLE中假设先验概率是等值的。而在MAP中，则是求

![](assets/1637136132-02b553ee97e64fc3eed4f34f46359da1.png)

使

![](assets/1637136132-09d2ca8339606465784b4cdc25b8d5b8.png)

的值最大，这也就是要求θ\\thetaθ值不仅仅是让似然函数最大，同时要求

![](assets/1637136132-02b553ee97e64fc3eed4f34f46359da1.png)

本身出现的先验概率也得比较大。可以认为MAP是贝叶斯算法的一种应用。

![](assets/1637136132-7ef31749d4db1931eccb35910ceab130.png)

## 2、EM算法

EM算法（Expectation Maximization Algorithm，最大期望算法）是一种迭代类型的算法，是一种在概率模型中寻找参数最大似然估计或者最大后验估计的算法，其中概率模型依赖于无法观测的隐藏变量。

##### EM算法流程：

*   初始化分布参数
*   重复下列两个操作直到收敛：
*   E步骤：估计隐藏变量的概率分布期望函数
*   M步骤：根据期望函数重新估计分布参数

##### EM算法原理：

给定的m个训练样本

![](assets/1637136132-71300e2c5419102663cdadb296a39eae.png)

，样本间独立，找出样本的模型参数θθθ，极大化模型分布的对数似然函数如下：

![](assets/1637136132-d904a42479f687b471e80a758254dc9c.png)

假定样本数据中存在隐含数据

![](assets/1637136132-d65c17ac387cccb30527c1ee1ac8ee5f.png)

，此时极大化模型分布的对数似然函数如下：

![](assets/1637136132-7eda9a6e52431b944b441bfebf88a7a9.png)

令z的分布为

![](assets/1637136132-4ca3ddec33c2189d0b09800aae42482c.png)

，并且

![](assets/1637136132-7c3ced022653017f3f5c3e1808600747.png)

，那么有如下公式：

![](assets/1637136132-0e40cc0cf79dbfd1ee948de2927c2f3c.png)

##### Jensen不等式：

如果函数f为凸函数，那么存在下列公式：

![](assets/1637136132-445ea17bdf8ab9b585d0a0871b869f0b.png)

如下图所示：

![](assets/1637136132-d5978cb8ecc6a1696a58801d661fb523.png)

若

![](assets/1637136132-bcfe333b5599e76d239a15ffbaa9a1e2.png)

则

![](assets/1637136132-09a07b70391fc664c293a62b29e13760.png)

根据Jensen不等式的特性，当下列式子的值为常数的时候，

![](assets/1637136132-f8c01ae76dad007c1fe03f0e37bbc40c.png)

函数才能取等号。

![](assets/1637136132-46a3474fd69bce23a8432322fd654f10.png)

##### EM算法流程：

样本数据

![](assets/1637136132-8abb880b38f41d65117173aeed3a2e38.png)

，联合分布

![](assets/1637136132-dc679319153dc4e9dcf5d72150fb58be.png)

，条件分布

![](assets/1637136132-6bf267ef0704b3b8d0481c2f8f43c9e4.png)

，最大迭代次数J。

*   随机初始化模型参数θ的初始值θ0θ\_0θ0​
*   开始EM算法的迭代处理：
*   E步：计算联合分布的条件概率期望

![](assets/1637136132-8ad452f90e35e54ded10ae8f4e0c1fb5.png)

*   M步：极大化L函数，得到

![](assets/1637136132-281aa5ea275657b829a119ceafe181a5.png)

![](assets/1637136132-73a460dab3384a10fc64b052f464f6e5.png)

*   如果

![](assets/1637136132-9effc845b91ab5aa6adf5fa86ce2ef3f.png)

已经收敛，则算法结束，输出最终的模型参数θ\\thetaθ，否则继续迭代处理

##### EM算法收敛证明

EM算法的收敛性只要我们能够证明对数似然函数的值在迭代的过程中是增加的即可，即证明下式成立：

![](assets/1637136132-e5237dacbe76b065d27a52e398ef12af.png)

证明过程如下：

![](assets/1637136132-f576e0f0c1acd0594f2d730b52d99f0a.png)

## 3、高斯混合模型

GMM（Gaussian Mixture Model，高斯混合模型）是指该算法由多个高斯模型线性叠加混合而成。每个高斯模型称之为component。GMM算法描述的是数据的本身存在的一种分布。 GMM算法常用于聚类应用中，component的个数就可以认为是类别的数量。假定GMM由k个Gaussian分布线性叠加而成，那么概率密度函数如下：

![](assets/1637136132-7df5f0bef07fa5e12d982119932bd447.png)

![](assets/1637136132-750666584467489905a19ef82d2da21a.png)

*   对数似然函数：

![](assets/1637136132-8cd049142022dc4a2aaac42d4716764c.png)

*   E step：

![](assets/1637136132-df5645dabd9014bcd1eeedac9c1f6197.png)

*   M step：

![](assets/1637136132-337650a5e32f001f8739ab616365cb1b.png)

*   对均值求偏导：

![](assets/1637136132-16970547768dbd97bb53a232e1af471c.png)

*   对方差求偏导：

![](assets/1637136132-c7914fe19da8f7f3c15b12a8db931bcf.png)

*   对概率使用拉格朗日乘子法求解：

本文参与[腾讯云自媒体分享计划](https://cloud.tencent.com/developer/support-plan)，欢迎正在阅读的你也加入，一起分享。

[举报](javascript:)

点赞 2分享

### 我来说两句

0 条评论

[登录](javascript:) 后参与评论